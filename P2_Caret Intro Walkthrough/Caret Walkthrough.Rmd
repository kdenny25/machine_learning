---
title: "Caret Package Walkthrough"
output: html_notebook
---

##Loading the Dataset##

> First thing is first, we need to load the data. This walkthrough introduces the **mlbench** package. Mlbench is a collection of artificial and real-world machine learning benchmark problems, including several data sets from the UCI repository. Which is great. This just makes life easier when practicing machine learning.

> This walkthrough uses the **Sonar** data set. It consists of 208 data points collected on 60 predictors. The goal is to predict the two classes **M** for metal cylinder or **R** for cylinder shaped rock.

**Split the Data into Two Groups**

```{r}
library(caret)
library(mlbench)
data(Sonar)

set.seed(107)
inTrain <- createDataPartition(
  y = Sonar$Class,
  ## the outcome data are needed
  p = .75,
  ## the percentage of data in the training set
  list = FALSE
)
## The format of the results

## The output is a set of integers fro the rows of Sonar
## that belong in the training set.
str(inTrain)
```

> **createDataPartition** does a stratified random split of the data.

```{r}
training <- Sonar[ inTrain, ]
testing  <- Sonar[-inTrain, ]

nrow(training)
#> [1] 157
nrow(testing)
#> [1] 51
```

> Next a partial least squares discriminant analysis (PLSDA) model will be tudned over the number of PLS components that should be retained.

```{r}
plsFit <- train( Class ~., data = training, method = "pls",
                 ## Center and scale the predictors for the training
                 ## set and all future samples.
                 preProc = c("center", "scale")
                 )
```


> At this point the model can be customized. The walkthrough recommends expanding the PLS models, changing the type of resample from bootstrap to three repeats of 10-fold cross-validation and changing the methods for measuring performance. Normally the root mean square error and R^2 are computed, but it will need to change to estimate the area under the ROC curve. 

> tuneLength=15 is used to evaluate all integers between 1 and 15.

```{r}
plsFit <- train(Class ~., data = training, method = "pls", preProc = c("center", "scale"),
                ## added:
                tuneLength = 15
                )
```

> Next, the resampling method needs to be changed. **trainControl** is used to do this. **Boot** is the default and needs to be changed to **repeatedcv**. For repeated cross validation.

```{r}
ctrl <- trainControl(method = "repeatedcv", repeats = 3)

plsFit <- train(Class ~., data = training, method = "pls", preProc = c("center", "scale"),
                tuneLength = 15,
                ## added:
                trControl = ctrl
                )
```


> Now a different method of measuring the performance is used.

```{r}
ctrl <- trainControl(
  method = "repeatedcv",
  repeats = 3,
  classProbs = TRUE,
  summaryFunction = twoClassSummary
)

set.seed(123)
plsFit <- train(Class~., data = training, method = "pls", preProc = c("center", "scale"),
                tuneLength = 15, trControl = ctrl, metric = "ROC"
                )
plsFit
```

> The output is the average resampled estimates of performance. At the bottom where it states the final value used for the model is 3. 3 being the most optimal resampling rate.

> Displaying the result of the training in a **ggplot** can show the relationship to the performance of cross-validation and the number of PLS components.

```{r}
ggplot(plsFit)
```

> Testing the training data comes next. The data that was partitioned from the original batch and labeled testing will be used to see if this model can predict new samples.

```{r}
plsClasses <- predict(plsFit, newdata = testing)
str(plsClasses)
#> Factor w/ 2 levels "M", "R": 2 1 1 1 1 2 2 1 2 2 2...
plsProbs <- predict(plsFit, newdata = testing, type = "prob")
head(plsProbs)
```

> The confusion matrix can break down the statistics for the model fit.

```{r}
confusionMatrix(data = plsClasses, testing$Class)
```

> Minor changes can be made to the **train** function to invoke new models. Below will utilize the regularized discriminant model.

```{r}
## To illustrate, a custom grid is used
rdaGrid = data.frame(gamma = (0:4)/4, lambda = 3/4)
set.seed(123)
rdaFit <- train(Class~., data = training, method = "rda", tuneGrid = rdaGrid,
                trControl = ctrl, metric = "ROC"
                )
rdaFit

rdaClasses <- predict(rdaFit, newdata = testing)
confusionMatrix(rdaClasses, testing$Class)
```

> The two models can be compared using the **resamples** function.

```{r}
resamps <- resamples(list(pls = plsFit, rda = rdaFit))
summary(resamps)
```


