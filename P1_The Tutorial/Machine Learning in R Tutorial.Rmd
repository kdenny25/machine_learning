---
title: "First Machine Learning Project Step-By-Step"
output: html_notebook
---

## Getting Started

> This is just me doing the step-by-step tutorial of machine learning with R from machinelearningmastery.com. 

**Intall Packages**

```{r}
install.packages("caret")
```

> Caret is short for Classification and Regression Training. It comes with a set of functions to create predictive models. This includes:

+ Data Splitting
+ Pre-Processing
+ Feature Selection
+ Model Tuning Using Resampling
+ Variable Importance Estimation

**Load the Library**

```{r}
library(caret)
```

**Load The Data**

> This tutorial uses the iris flowers dataset. This is the "hello world" of machine learning data sets so and is actually fairly easy to load with R Studio, but for the sake of a real life experience I will load the data via a downloaded CSV file and load it that way.

```{r}
# define the filename
filepath <- paste(getwd(), "/raw_data/iris.csv", sep="")
# load the CSV file from the local directory
dataset <- read.csv(filepath, header=FALSE)
# set the column names in the dataset
colnames(dataset) <- c("Sepal.Length","Sepal.Width","Petal.Length","Petal.Width","Species")
```

> I altered the code a little bit to account for the filepath, so I pasted the working directory with the rest of the filepath. Be sure to check your working directory prior to running the code.

**Create a Validation Dataset**

> So this part is just breaking up the data so that we have some data for the training dataset and other data for the validation. The validation is meant to determine how accurate the best model is. So in this tutorial 80% of the data is used for training, while 20% is for validation

```{r}
# create a list of 80% of the rows in the original dataset we can use for training
validation_index <- createDataPartition(dataset$Species, p=0.80, list=FALSE)
# select 20% of the data for validation
validation <- dataset[-validation_index,]
# use the remaining 80% of data to train and test the models
dataset <- dataset[validation_index,]
```

> The original dataset had 150 objects. Looking at the global environment window I now see that dataset has 120 objects and validation has only 30. So it worked.

**Summarize Dataset**

> This step is a guide on how to look through the data and get a visual on it. I did find that some packages required for this part were not installed on my system yet so I had to install them. So I'm pretty sure you need ggplot and ellipse. Not sure if you need anything else.

> These are the different ways we will be looking at the data

+ Dimensions of the dataset
+ Types of the attributes
+ Peek at the data itself
+ Levels of the class attribute
+ Breakdown of the instances in each class
+ Statistical sumary of all attributes

**Dimensions of Dataset**

```{r}
# dimensions of dataset
dim(dataset)
```

> You should see 120 instances and 5 attributes. This information can also be found in the global environment window on the right.

**Types of Attributes**

> Knowing the types of the attributes is a good idea, because it can help identify which types of transforms you might need in order to prepare the data before modeling it.

```{r}
# list types for each attribute
sapply(dataset, class)
```

> If done correctly you should see four "numeric" types and one "factor" type. 

**Peek at the Data**

> This will give you an idea of how the data looks. Keep in mind that it only shows the first 5 rows.

```{r}
# take a peek at the first 5 rows of the data
head(dataset)
```

**Levels of the Class**

> The class variable is a factor. A factor is a class that has multiple class labels or levels. We can check to see how many levels there are and what they are

```{r}
# list the levels for the class
levels(dataset$Species)
```

> If done correctly you will see 3 different labels. These are the levels.

**Class Distribution**

> Class distribution will show the number of rows associated to each class. In this example we will see the total number of each along with a percentage of the total  number of objects.

```{r}
# summarize the class distribution
percentage <- prop.table(table(dataset$Species)) * 100
cbind(freq=table(dataset$Species), percentage=percentage)
```

**Statistical Summary**

> Now we look at the summary this will show the mean, min and max values and percentiles.

```{r}
# summarize attribute distribution
summary(dataset)
```

**Visualize Dataset**

> Looking at numbers is one thing, but vizualizing the data can help create a better picture. This tutorial walks through two types of plots, the univariate plot and the multivariate plot.

**Univariate Plots**

> Univariate plots are used to plot individual variables. With this example we split the data up into input attributes (x) and output attributes (y). 

```{r}
# split input and output
x <- dataset[,1:4]
y <- dataset[,5]
```

Once we separate the attributes, the data can then be used to create a box and whisker plot.

```{r}
# boxplot for each attribute on one image
par(mfrow=c(1,4))
  for(i in 1:4) {
    boxplot(x[,i], main=names(iris)[i])
  }
```

> The tutorial then goes on to create a barplot, but since there are an even number of each output the isn't much to decern from the plot.

```{r}
# barplot for class breakdown
plot(y)
```


**Multivariate Plots**

>These show the interactions between the variables. Before jumping into this next plot, I ran into an issue where it was not showing up correctly. The simple solution was needing to install the "ellipse" package. I'll go ahead and put that in below.

```{r}
# install ellipse package
install.packages("ellipse")
```

> Once installed you can now create the scatterplot matrix.

```{r}
# scatterplot matrix
featurePlot(x=x, y=y, plot="ellipse")
```

> We can also look at a multivariate box and whisker plot. This time it breaks down each input variable.

```{r}
# box and whisker plots for each attribute
featurePlot(x=x, y=y, plot="box")
```

> Next we can look at density plots for each attribute

```{r}
# density plots fro each attribute by class value
scales <- list(x=list(relation="free"), y=list(relation="free"))
featurePlot(x=x, y=y, plot="density", scales=scales)
```

**Evaluate Some Algorithms**

>This part we create some models and estimate their accuracy on unseed data. The steps covered are:

1. Set-up the test harness to use 10-fold cross validation.
2. Build 5 different models to predict specieis from flower measurements.
3. Select the best model.

>10-fold crossvalidation is used to estimate the accuracy. This means the data will be split into 10 parts. 9 will be trained and 1 will be tested. This process will be repeated 3 times for each algorithm.

```{r}
# Run algorithms using 10-fold cross validation
control <- trainControl(method="cv", number=10)
metric <- "Accuracy"
```

>The metric of "Accuracy" is used to evaluate the models. This is a ratio of the number of correctly predicted instances in, divided by the total number of instances in the dataset, multiplied by 100 to give a percentage.

**Build Models**

>5 different algorithms are evaluated in this tutorial.

+ Linear Discrimant Analysis (LDA)
+ Classification and Regression Trees (CART)
+ k-Nearest Neighbors (kNN)
+ Support Vector Machines (SVM) with a linear kernal
+ Random Forest (RF)

> The random seed is reset before each run to ensure that the valuation of each algorithm is performed using exactly the same data splits. It ensures the results are directly comparable.

> Before moving forward make sure you have the e1071, kernlab, randomForest,package installed

```{r}
install.packages("e1071", "kernlab", "randomForest")
```

> Once installed, you should be ready to run the below code.


```{r}
# a) linear algorithms
set.seed(7)
fit.lda <- train(Species~., data=dataset, method="lda", metric=metric, trControl=control)
# b) nonlinear algorithms
# CART
set.seed(7)
fit.cart <- train(Species~., data=dataset, method="rpart", metric=metric, trControl=control)
#kNN
set.seed(7)
fit.knn <- train(Species~., data=dataset, method="knn", metric=metric, trControl=control)
# c) advanced algorithms
#SVM
set.seed(7)
fit.svm <- train(Species~., data=dataset, method="svmRadial", metric=metric, trControl=control)
# Random Forest
set.seed(7)
fit.rf <- train(Species~., data=dataset, method="rf", metric=metric, trControl=control)
```

> Now we need to report on the models to determine the more accurate one.

```{r}
# summarize accuracy of models
results <- resamples(list(lda=fit.lda, cart=fit.cart, knn=fit.knn, svm=fit.svm, rf=fit.rf))
summary(results)
```

> Results are then summarized using plots

```{r}
# compare accuracy of models
dotplot(results)
```

